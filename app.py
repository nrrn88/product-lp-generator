import streamlit as st
import scraper
import importlib
import prompts
importlib.reload(prompts) # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å¤‰æ›´ã‚’å¼·åˆ¶çš„ã«åæ˜ 
import os
import re
import json
from datetime import datetime

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="å•†å“LPè‡ªå‹•ç”Ÿæˆãƒ„ãƒ¼ãƒ« (SEO/AIOå¼·åŒ–ç‰ˆ)",
    page_icon="ğŸ’Š",
    layout="wide"
)

# ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜
st.title("ğŸ’Š åŒ»è–¬å“EC å•†å“è©³ç´°ãƒšãƒ¼ã‚¸è‡ªå‹•ç”Ÿæˆãƒ„ãƒ¼ãƒ«")
st.markdown("""
ç«¶åˆã‚µã‚¤ãƒˆã®URLã‚’å…¥åŠ›ã—ã¦ã€SEOãƒ»AIOï¼ˆAI Overviewï¼‰ã«æœ€é©åŒ–ã•ã‚ŒãŸå•†å“ãƒšãƒ¼ã‚¸HTMLã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã™ã€‚
""")

# APIã‚­ãƒ¼å…¥åŠ›ï¼ˆãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ã«ç§»å‹•ï¼‰
st.info("ğŸ‘‡ ã“ã“ã«å…ˆã»ã©ã‚³ãƒ”ãƒ¼ã—ãŸã€ŒAPIã‚­ãƒ¼ã€ã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„")
api_key = st.text_input("Gemini API Key", type="password", help="Google AI Studioã§å–å¾—ã—ãŸAPIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
if not api_key:
    st.warning("âš ï¸ APIã‚­ãƒ¼ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“")
else:
    st.success("APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¾ã—ãŸï¼")

    st.markdown("---")
    
    st.caption("ğŸ¤– ãƒ¢ãƒ‡ãƒ«è¨­å®š")
    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®é¸æŠ
    text_model = st.selectbox(
        "ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«",
        options=[
            "gemini-3-pro-preview",
            "gemini-2.5-pro",
            "gemini-2.5-flash",
            "gemini-2.0-flash",
            "gemini-1.5-pro-002"
        ],
        index=0,
        help="HTMLã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«"
    )
    





st.markdown("---")

def parse_generated_content(text):
    """
    ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡ºã™ã‚‹
    """
    # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
    if text.startswith("Error:"):
        return {"error": text}

    sections = {}
    
    # æ­£è¦è¡¨ç¾ã§ã‚¿ã‚°ã®ä¸­èº«ã‚’æŠ½å‡º
    patterns = {
        "metadata": r"<metadata>(.*?)</metadata>",
        "html_content": r"<html_content>(.*?)</html_content>",
        "reviews": r"<reviews>(.*?)</reviews>",
        "references": r"<references>(.*?)</references>"
    }
    
    for key, pattern in patterns.items():
        match = re.search(pattern, text, re.DOTALL)
        if match:
            sections[key] = match.group(1).strip()
        else:
            sections[key] = ""
            
    return sections

# ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
col1, col2 = st.columns([1, 1])

with col1:
    st.subheader("1. æƒ…å ±å…¥åŠ›")
    
    product_name = st.text_input("å•†å“å", placeholder="ä¾‹: ã‚¢ãƒŠãƒ‰ãƒªãƒ³")
    
    target_urls = st.text_area(
        "å‚è€ƒã«ã™ã‚‹ç«¶åˆURL (è¤‡æ•°å¯)", 
        height=150, 
        placeholder="https://example.com/product/a\nhttps://competitor.com/item/b"
    )
    
    additional_info = st.text_area("ç‰¹è¨˜äº‹é … (ä»»æ„)", placeholder="ä¾‹: æˆåˆ†é‡ã¯50mgã§ã™ã€‚é…é€ã¯1é€±é–“ç¨‹åº¦ã§ã™ã€‚")

    if st.button("ğŸš€ ãƒšãƒ¼ã‚¸ã‚’ç”Ÿæˆã™ã‚‹", type="primary", disabled=not api_key):
        if not product_name or not target_urls:
            st.error("å•†å“åã¨URLã¯å¿…é ˆã§ã™ã€‚")
        else:
            with st.status("å‡¦ç†ã‚’å®Ÿè¡Œä¸­...", expanded=True) as status:
                # 1. ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                st.write("ğŸŒ ç«¶åˆã‚µã‚¤ãƒˆã‹ã‚‰æƒ…å ±ã‚’åé›†ä¸­...")
                scrape_results = scraper.scrape_multiple_urls(target_urls)
                
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ä½œæˆ
                context_text = ""
                for res in scrape_results:
                    if "error" in res:
                        st.warning(f"å–å¾—å¤±æ•—: {res['url']} ({res['error']})")
                    else:
                        st.success(f"å–å¾—æˆåŠŸ: {res['title']}")
                        context_text += f"\n--- Source: {res['url']} ---\nTitle: {res['title']}\nContent: {res['content']}\n"
                
                if additional_info:
                    context_text += f"\n--- User Note ---\n{additional_info}\n"

                # 2. AIç”Ÿæˆ
                st.write(f"ğŸ§  AI ({text_model}) ãŒæ§‹æˆã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆä¸­ (SEO/AIOå¯¾ç­–)...")
                raw_response = prompts.generate_content(api_key, context_text, product_name, text_model)
                
                st.session_state['raw_response'] = raw_response
                st.session_state['product_name'] = product_name
                
                status.update(label="å®Œäº†!", state="complete", expanded=False)

with col2:
    st.subheader("2. ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ & å‡ºåŠ›")
    
    if 'raw_response' in st.session_state:
        raw_text = st.session_state['raw_response']
        parsed_data = parse_generated_content(raw_text)
        
        # ã‚¨ãƒ©ãƒ¼åˆ¤å®š
        if "error" in parsed_data:
            st.error(parsed_data["error"])
        else:
            # ã‚¿ãƒ–ã§è¡¨ç¤ºåˆ‡ã‚Šæ›¿ãˆ
            tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ–¼ï¸ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ğŸ“ HTML", "âš™ï¸ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿", "â­ ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ğŸ”— å‚è€ƒãƒªãƒ³ã‚¯"])
            
            html_content = parsed_data.get("html_content", "")
            
            with tab1:
                st.caption("â€»ã‚¹ã‚¿ã‚¤ãƒ«ã¯ç°¡æ˜“çš„ãªã‚‚ã®ã§ã™ã€‚")
                if html_content:
                    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã«èƒŒæ™¯è‰²ã‚’ç™½ã«å›ºå®šã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼ã‚’è¿½åŠ 
                    preview_html = f"""
                    <div style="background-color: #ffffff; color: #333333; padding: 20px; border-radius: 5px;">
                        {html_content}
                    </div>
                    """
                    st.components.v1.html(preview_html, height=600, scrolling=True)
                else:
                    st.warning("HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            
            with tab2:
                st.text_area("HTML Source", html_content, height=400)
                
                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
                file_name = f"{product_name}_{datetime.now().strftime('%Y%m%d')}.html"
                st.download_button(
                    label="ğŸ’¾ HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=html_content,
                    file_name=file_name,
                    mime="text/html"
                )
            
            with tab3:
                metadata_text = parsed_data.get("metadata", "")
                
                # ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã€H1ã‚’æŠ½å‡º
                title_match = re.search(r"Recommended Title:\s*(.*)", metadata_text)
                h1_match = re.search(r"Recommended H1:\s*(.*)", metadata_text)
                desc_match = re.search(r"Recommended Description:\s*(.*)", metadata_text)
                
                rec_title = title_match.group(1).strip() if title_match else ""
                rec_h1 = h1_match.group(1).strip() if h1_match else ""
                rec_desc = desc_match.group(1).strip() if desc_match else ""

                st.subheader("æ¨å¥¨ã‚¿ã‚¤ãƒˆãƒ«")
                st.code(rec_title, language=None)
                
                st.subheader("æ¨å¥¨H1")
                st.code(rec_h1, language=None)
                
                st.subheader("æ¨å¥¨ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³")
                st.code(rec_desc, language=None)
                
                st.markdown("---")
                st.subheader("æ¨å¥¨ç”»åƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
                st.caption("ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä»–ã®ç”»åƒç”Ÿæˆãƒ„ãƒ¼ãƒ«ï¼ˆMidjourneyã€DALL-E3ãªã©ï¼‰ã§ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")

                st.caption("ğŸ¨ æŠ½è±¡ã‚¤ãƒ¡ãƒ¼ã‚¸ (åŠ¹æœãƒ»æ‚©ã¿è§£æ±º)")
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŠ½å‡ºã¯ã—ã¦ã„ãªã„ã®ã§ã€metadataå…¨ä½“ã‹ã‚‰å‚ç…§ã™ã‚‹ã‹ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«Rawãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã¦ã‚‚ã‚‰ã†
                # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«æ­£è¦è¡¨ç¾ã§å†æŠ½å‡ºã™ã‚‹
                img_abstract_match = re.search(r"(?:-|\*)\s*(?:\*\*)?\[Abstract\](?:\*\*)?:?\s*(.*)", metadata_text, re.IGNORECASE)
                if img_abstract_match:
                    st.code(img_abstract_match.group(1).strip(), language=None)

                st.caption("ğŸ˜Š äººç‰©ã‚¤ãƒ¡ãƒ¼ã‚¸ (ä¿¡é ¼æ„Ÿãƒ»ç¬‘é¡”)")
                img_person_match = re.search(r"(?:-|\*)\s*(?:\*\*)?\[Person\](?:\*\*)?:?\s*(.*)", metadata_text, re.IGNORECASE)
                if img_person_match:
                    st.code(img_person_match.group(1).strip(), language=None)
                
                with st.expander("å…¨ã¦ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ & ç”Ÿãƒ‡ãƒ¼ã‚¿"):
                    st.text_area("Raw Metadata", metadata_text, height=200)
                
            with tab4:
                reviews_text = parsed_data.get("reviews", "")
                st.caption("ç”Ÿæˆã•ã‚ŒãŸJSONãƒ‡ãƒ¼ã‚¿")
                st.code(reviews_text, language="json")
                
                # JSONãƒ‘ãƒ¼ã‚¹ã‚’è©¦ã¿ã‚‹
                try:
                    reviews_json = json.loads(reviews_text)
                    st.markdown("#### ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                    for rev in reviews_json:
                        with st.expander(f"{rev.get('rating', '5')}â­ {rev.get('title', 'No Title')} ({rev.get('name', 'Anonymous')})"):
                            st.write(rev.get('body', ''))
                            st.caption(f"æ—¥ä»˜: {rev.get('date', '')}")
                except:
                    st.warning("ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆå½¢å¼ãŒå´©ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰ã€‚")
            
            with tab5:
                ref_text = parsed_data.get("references", "")
                st.markdown(ref_text)
        

        


    else:
        st.info("å·¦å´ã®ãƒ•ã‚©ãƒ¼ãƒ ã«å…¥åŠ›ã—ã¦ç”Ÿæˆãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")

